{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8479f56f",
   "metadata": {},
   "source": [
    "# Tutorial: Using The OpenAI API for Categorization and Labeling\n",
    "### Author: Campbell Lund\n",
    "### 9/26/2023\n",
    "This notebook walks through how to get started using the OpenAI API. We use the specific example of labeling and categorizing sentences to illustrate the capabilities and techniques, such as one-shot learning and chain of thought prompting, of `gpt-3.5-turbo` for text analysis.\n",
    "\n",
    "### Table of contents:\n",
    "- 1. [Initialization](#sec1)\n",
    "- 2. [Example: prompting the model and zero-shot learning](#sec2)\n",
    "    - 2.1.[Converting text to Python lists](#sec2p1)\n",
    "- 3. [Example: prompting the model and one-shot learning](#sec3)\n",
    "- 4. [Example: prompting in batches](#sec4)\n",
    "    - 4.1.[Determining unique categories](#sec4p1)\n",
    "- 5. [Categorizing](#sec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa786973",
   "metadata": {},
   "source": [
    "## 1. Initialization <a name=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8e978",
   "metadata": {},
   "source": [
    "Import or `!pip install` the following libraries. For security, I've stored my API key in a `.env` file since this notebook will be shared. Instructions for generating your personal API key can be found [here.](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) If you don't wish to store your key in a `.env` file, simply set `openai.api_key` equal to your key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "# for exponential backoff\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  \n",
    "# retrieving our API key from a secure file\n",
    "#from dotenv import load_dotenv, find_dotenv\n",
    "#_ = load_dotenv(find_dotenv())\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "openai.api_key  = # YOUR KEY HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4ba44",
   "metadata": {},
   "source": [
    "### helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the model's response to a given message query\n",
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, # degree of randomness\n",
    "                                 max_tokens=150): #4000 is max for input and response combined\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data/allQueries.csv', header=None, names=[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fefbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0100d58",
   "metadata": {},
   "source": [
    "## 2. Ex: prompting the model and zero-shot learning <a name=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3560eb5",
   "metadata": {},
   "source": [
    "Now that we have a helper function to send queries and receive responses from the model, we must engineer our prompt. This is where trial and error is really your friend. The model can handle fairly complex instructions, but it's best to be direct. My advice for engineering a successful prompt is to pretend you're writing pseudo-code rather than giving written instructions to a friend - remember it's a computer you're training, not a person.\n",
    "\n",
    "### Vocab:\n",
    "- **zero-shot learning:** a ML paradigm for when a model is applied to objects or concepts it has never seen in training. Since we do not provide labeled examples to ChatGPT for fine-tuning the model, the below is an example of zero-shot learning. \n",
    "- **delimiter:** a character used to indicate the start of a new message.\n",
    "- **token:** a unit of text that the model processes. Tokens are usually individual words, but complex words may be made up of multiple tokens. For our purposes, think of tokens as the number of words in a query or a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a good delimiter since it counts as a single token and isn't likely to appear naturally in the message\n",
    "delimiter = \"####\" \n",
    "\n",
    "# message given to the model with instructions for how to respond\n",
    "system_message = f\"\"\"\n",
    "Your job is to determine the topic of a given sentence. \\\n",
    "You will be given a sentence as input and you will return \\\n",
    "a single word that best represents the topic of the sentence. \\\n",
    "Each input will be delimited by {delimiter} characters. \\\n",
    "Output a Python list of objects where each object has the following format: \\\n",
    "    \"Sentence\": <the input sentence>, \\\n",
    "    \"Topic\": <the topic output> \\\n",
    "\"\"\"\n",
    "# message input from the user\n",
    "user_message = f\"\"\"\\\n",
    "rock melting in india \\\n",
    "{delimiter}\n",
    "free energy machines \\\n",
    "{delimiter}\n",
    "levitation devices \\\n",
    "\"\"\"\n",
    "\n",
    "# combining the system and user messages to give as input to our helper function\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cac335",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion_from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d745a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f7c3d",
   "metadata": {},
   "source": [
    "Although the output apprears to be a Python list as we instructed the model, note that all output from our helper function will be a string which we must convert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a4ac20",
   "metadata": {},
   "source": [
    "## 2.1. Converting text to Python sets <a name=\"sec2p1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2384ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710d779",
   "metadata": {},
   "source": [
    "Now we have an actual array of objects!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527478e",
   "metadata": {},
   "source": [
    "## 3. Ex: prompting the model and one-shot learning <a name=\"sec3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4101415b",
   "metadata": {},
   "source": [
    "For more complex tasks, or if the model just isn't returning what you expect, examples may need to be provided. \n",
    "### Vocab:\n",
    "- **one-shot learning:** a ML paradigm for when a model is trained to handle objects or concepts based on a very limited amount of training data. Below, we provide two examples of input and expected output to fine-tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2bbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#providing examples of correct outputs to improve model accuracy\n",
    "examples = [\n",
    "    {\"role\": \"user\", \"content\": \"rock melting in india\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"{\\\"Sentence\\\": \\\"rock melting in india\\\", \\\"Topic\\\": \\\"melting\\\"}\"},\n",
    "    {\"role\": \"user\", \"content\": \"free energy machines\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"{\\\"Sentence\\\": \\\"free energy machines\\\", \\\"Topic\\\": \\\"machines\\\"}\"}\n",
    "      ]\n",
    "    \n",
    "# using the same user system and message as defined in section 2\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# providing both the examples and the previous messages as input\n",
    "response = get_completion_from_messages(examples + messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda81380",
   "metadata": {},
   "source": [
    "Notice how the third topic changes after providing examples for the first two inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b7a93",
   "metadata": {},
   "source": [
    "## 4. Ex: prompting in batches <a name=\"sec4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fa8181",
   "metadata": {},
   "source": [
    "Often when we use the OpenAI API it is because we have large amounts of data that we want to use as prompts. Entering these queries by hand is time-consuming, so let's automate the process. Some important notes:\n",
    "\n",
    "Our API account has both a `rate_limit` and a `max_tokens` value. The `max_tokens` is 4000 tokens for both the user message and the generated response. This means the combined input and output for each query must be less than 4000 tokens. This is one of the reasons we will break up large tasks into smaller parts.\n",
    "\n",
    "Another reason is to stay within the `rate_limit`. To determine the rate limit of your account, simply try running the old`get_completion_from_messages()` helper function on a large data frame. It won't be long until you receive this message:\n",
    "\n",
    "`RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-KiUYu8NRxzHi3TljuvYUEiIG on tokens per min. Limit: 90000 / min. Current: 87379 / min. Contact us through our help center at help.openai.com if you continue to have issues.`\n",
    "\n",
    "Now we know our `rate_limit` is 9000 tokens/min."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23bc01",
   "metadata": {},
   "source": [
    "### helper function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc2858",
   "metadata": {},
   "source": [
    "The below helper function is similar to the previous `get_completion_from_messages()` except now we ask for a `df`, `system_message`, and `batch_size` as parameters. `get_completion_from_messages_batch()` will query each row of the provided `df` in batches of `batch_size` to the model and return an array of all responses. If your results are inaccurate, try lowering `batch_size` and if it's taking too long, raise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an array containing the model's responses from a given df of message queries\n",
    "@retry(wait=wait_random_exponential(min=30, max=60), stop=stop_after_attempt(6))\n",
    "def get_completion_from_messages_batch(df,\n",
    "                                 system_message,\n",
    "                                 batch_size,\n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, # degree of randomness\n",
    "                                 max_tokens=300): # 4000 is max for input and response combined\n",
    "    \n",
    "    responses = []\n",
    "    delimiter = '####'\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        user_message = \"\"\n",
    "        for index, row in batch.iterrows():\n",
    "            user_message += f\"{row['sentences']}{delimiter}\"\n",
    "            \n",
    "        messages = [  \n",
    "        {'role':'system', \n",
    "         'content': system_message},    \n",
    "        {'role':'user', \n",
    "         'content': user_message}  \n",
    "        ] \n",
    "        \n",
    "        # calculate sleep time before each request to ensure we don't exceed the rate limit\n",
    "        # calculate_sleep_time() # comment out to test your account's rate limit\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature, \n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message[\"content\"]\n",
    "        responses.append(content) \n",
    "        \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c28a40",
   "metadata": {},
   "source": [
    "### engineering a new prompt:\n",
    "This prompt will be applied to our entire `df`. Try to keep it simple to speed things up. Since LLMs are already trained to do summarization tasks, we'll start our categorization by asking the model to determine the subject of each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "    The goal is to classify sentences based on their topic.\\\n",
    "    Your job is to determine the topic of a given sentence. \\\n",
    "    You will be given a sentence as input and you will return \\\n",
    "    a single word that best represents the topic of the sentence. \\\n",
    "    Be broad with the topics, some sentences should share \\\n",
    "    similar topics and it is okay to return the same topic\\\n",
    "    multiple times. Each input will be delimited by {delimiter} \\\n",
    "    characters. Format your response as a Python list, each \\\n",
    "    topic must be in double quotations. \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd58d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = get_completion_from_messages_batch(df, system_message, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bd747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format string\n",
    "formatted_responses = []\n",
    "for i, r in enumerate(responses):\n",
    "    if r[len(r)-1] != \"]\":\n",
    "        responses[i] += \"]\"\n",
    "    print(responses[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7836409",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b54944",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f7c87",
   "metadata": {},
   "source": [
    "## 4.1. Determining unique categories <a name=\"sec4p1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading allTopics.csv - run this cell only if you're working with the saved data\n",
    "temp = pd.read_csv('data/allTopics.csv', skiprows=1, names=[\"topics\"])\n",
    "\n",
    "all_topics = []\n",
    "temp = temp.values.tolist()\n",
    "for t in temp:\n",
    "    all_topics.append(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = [string.lower() for string in all_topics if len(string.split()) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70879457",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_topics = list(set(all_topics))\n",
    "print('Number of unique topics: ', len(unique_topics))\n",
    "print('Topics: ', unique_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59880c3",
   "metadata": {},
   "source": [
    "Since there are a reasonable number of unique topics, I'll narrow down the most relevant final categories by hand. You can prompt the LLM to do this or use another NLP technique if you wish. I discerned 12 major categories as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9948fe",
   "metadata": {},
   "source": [
    "1. 'politics'\n",
    "    - 'government'\n",
    "    - 'scandal'\n",
    "    - 'misinformation'\n",
    "    - 'surveillance'\n",
    "    - 'crime'\n",
    "2. 'health'\n",
    "\t- 'pandemic'\n",
    "3. 'terrorism'\n",
    "\t- '9/11'\n",
    "    - 'tragedy'\n",
    "4. 'media'\n",
    "\t- 'entertainment'\n",
    "5. 'economy'\n",
    "\t- 'money'\n",
    "6. 'history'\n",
    "7. 'environment'\n",
    "\t- 'sustainability'\n",
    "8. 'science'\n",
    "\t- 'geology'\n",
    "9. 'technology'\n",
    "\t- 'aviation'\n",
    "10. 'conspiracy'\n",
    "\t- 'false'\n",
    "11. 'space'\n",
    "\t- 'paranormal'\n",
    "    - 'extraterrestrial'\n",
    "    - 'aliens'\n",
    "    - 'astronomy'\n",
    "    - 'ufos'\n",
    "12. 'supernatural'\n",
    "\t- 'cryptid'\n",
    "    - 'cryptozoology'\n",
    "    - 'mystery'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde67ab",
   "metadata": {},
   "source": [
    "## 5. Categorizing  <a name=\"sec5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3443e68e",
   "metadata": {},
   "source": [
    "In this section, we're using the output from a previous query as the input to another. This is called **chain of thought prompting**. For complex tasks, it's necessary to break problems down into digestible parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"\n",
    "    Your job is to classify sentences based on their topic.\\\n",
    "    Given a sentence, determine which category it belongs \\\n",
    "    to from the topic list. \\\n",
    "        Topic list: \\\n",
    "            [politics, \\\n",
    "            health, \\\n",
    "            terrorism, \\\n",
    "            media, \\\n",
    "            economy, \\\n",
    "            history, \\\n",
    "            environment, \\\n",
    "            science, \\\n",
    "            technology, \\\n",
    "            space, \\\n",
    "            supernatural] \\  \n",
    "    Each input will be delimited by #### characters. \\\n",
    "    Format your response as a Python list. Output a Python \\\n",
    "    object of the following format: \\\n",
    "    \"topic\": <the determined topic from the Topic List>, \\\n",
    "    \"sentence\": <the input sentence> \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = get_completion_from_messages_batch(df, system_message, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8431e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935dc39",
   "metadata": {},
   "source": [
    "Once you've fine-tuned the prompt to your liking, run `get_completion_from_messages_batch()` on your entire `df`. This will take a long time to compile. A way to speed it up is to try lowering the batch size, either with a loop as we did before in `Section 4.1` or manually slicing the df."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
